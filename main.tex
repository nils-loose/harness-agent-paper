\documentclass[sigconf,review,anonymous]{acmart}

\usepackage{listings}


\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}
\acmSubmissionID{123}



\begin{document}

\title{Multi-Agent Driven Fuzz-Harness Generation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \city{San Antonio}
  \state{Texas}
  \country{USA}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

\renewcommand{\shortauthors}{Trovato et al.}


\begin{abstract}
TODO: Klaus generate Abstract
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007</concept_id>
       <concept_desc>Software and its engineering</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006</concept_id>
       <concept_desc>Software and its engineering~Software notations and tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002978.10003022</concept_id>
       <concept_desc>Security and privacy~Software and application security</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Software and its engineering}
\ccsdesc[500]{Software and its engineering~Software notations and tools}
\ccsdesc[500]{Security and privacy~Software and application security}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{fuzzing, fuzz harness generation, large language models, multi-agent systems, automated software testing, program analysis, feedback-driven refinement, Java ecosystem, coverage-guided fuzzing, software security}


\maketitle

\section{Introduction}

Coverage-guided fuzzing has emerged as the dominant approach for discovering robustness vulnerabilities in systems software, libraries, and frameworks~\cite{CITE}. Yet its effectiveness on \emph{library code} fundamentally depends on the availability of high-quality \emph{fuzz harnesses}, i.e., specialized programs that bridge unstructured fuzzer inputs to structured API invocations. Manual harness construction remains a formidable barrier to scalable library fuzzing: developers must reverse-engineer API preconditions, synthesize realistic call sequences, and navigate complex build environments with transitive dependencies and environmental constraints. In ecosystems like Java, where libraries often require intricate Maven dependency graphs, reflection-based initialization, and runtime configuration, harness authoring has become the principal bottleneck limiting fuzzing coverage.

The research community has pursued three complementary strategies to automate harness generation. \emph{Usage-based approaches} mine API call patterns from existing client code or test suites, repurposing observed interactions as fuzzing drivers~\cite{DBLP:conf/sigsoft/BabicBCIKKLSW19:FUDGE, DBLP:conf/uss/IspoglouAMP20:FuzzGen, DBLP:conf/sp/JeongJYMKJKSH23:UTopia, DBLP:conf/icse/ZhangZLWLZJ23:Daisy, DBLP:journals/pacmse/WuNH25:WildSync}. While these methods capture realistic usage patterns, they inherently require rich consumer corpora, a condition rarely satisfied for niche libraries, legacy codebases, or newly released APIs. \emph{Structure- and API-based approaches} circumvent this limitation by deriving harnesses directly from type signatures, interface specifications, or dataflow graphs~\cite{DBLP:conf/icse/ZhangLMZ021:IntelliGen, DBLP:conf/icse/GreenA22:GraphFuzz, DBLP:conf/ccs/ChenXLWC23:Hopper, DBLP:conf/issta/XiongDCQWSZ24:Atlas, DBLP:journals/pacmse/ToffaliniBTP25:LibErator, DBLP:conf/icse/ShermanN25:OGHarn, DBLP:conf/ndss/0007ZLSZLQ25:NEXZZER}. However, these techniques struggle with non-obvious API contracts and often depend on domain-specific heuristics or human-in-the-loop guidance to resolve ambiguities. \emph{Feedback-driven approaches} close the loop through iterative refinement, using compilation errors, runtime exceptions, or coverage signals to guide harness evolution~\cite{DBLP:conf/uss/ZhangLZZZZXLL0H23:Rubick, DBLP:journals/pacmse/ToffaliniBTP25:LibErator, DBLP:conf/icse/ShermanN25:OGHarn, DBLP:conf/ndss/0007ZLSZLQ25:NEXZZER}. Despite their promise, existing feedback systems typically focus on narrow domains or lack robust mechanisms to automatically resolve build failures and environmental misconfigurations.

Large Language Models (LLMs) offer a path forward by combining code synthesis capabilities with learned knowledge of API usage patterns and debugging strategies. Early work has explored LLM-driven harness generation, revealing both opportunities and fundamental challenges: semantic drift under iterative refinement, brittleness to build system variations, and difficulties grounding generation in concrete coverage objectives~\cite{DBLP:conf/issta/ZhangZBLMXLSL24:HowEffectiveAreThey, DBLP:conf/sigsoft/Jiang0MCZSWFWLZ24:WhenFuzzingMeetsLLMs}. Recent systems have demonstrated progress through coverage-guided prompt evolution~\cite{DBLP:conf/ccs/LyuXCC24:PromptFuzz}, knowledge graph augmentation~\cite{DBLP:conf/icse/XuMZZCHLW25:CKGFuzzer}, and multi-agent coordination~\cite{DBLP:journals/corr/abs-2507-18289:Scheduzz}. However, a critical gap remains: no existing approach provides a fully automated, end-to-end pipeline that handles environment preparation, synthesis, compilation repair, and coverage-driven optimization without manual intervention.

We address this gap through a multi-agent system designed specifically for Java library fuzzing. Java serves as an ideal target for several reasons. First, its ecosystem comprises millions of Maven-hosted libraries that form the backbone of enterprise software infrastructure~\cite{CITE}, creating both high-impact fuzzing opportunities and a representative testbed for automation techniques. Second, Java's characteristics, i.e., deep dependency graphs, reflection-heavy APIs, and complex runtime requirements, stress-test the limits of automated harness generation, forcing confrontation with the very challenges that have hindered prior work. Third, solutions developed for Java's managed runtime and rich metadata naturally extend to similar ecosystems (Kotlin, Scala, .NET), amplifying broader impact.

We present a multi-agent architecture that orchestrates specialized LLM-based reasoning agents through a structured feedback loop. Our system decomposes harness generation into distinct phases (i.e., environment preparation, exploratory research, synthesis, validation, and refinement) each handled by dedicated agents equipped with domain-specific tools. Central to our approach is a tool-augmented reasoning framework that enables agents to query code structure, documentation, and implementation details on demand rather than relying on pre-computed summaries. Agents leverage Class Hierarchy Analysis to construct call graphs, retrieve API documentation from Javadoc archives, and examine source implementations—all through conversational exploration that adapts to the specific needs of each target library. Synthesis proceeds through iterative refinement guided by compilation diagnostics and coverage feedback, with separate agents specialized for generation (creative API exploration) and debugging (systematic error diagnosis). The result is a closed-loop workflow that autonomously produces compilable, coverage-optimized harnesses without requiring consumer code or manual intervention.

Our contributions advance the state of automated harness generation:
\begin{itemize}
    \item \textbf{A principled multi-agent architecture for end-to-end harness synthesis.} We introduce a workflow that decomposes harness generation into specialized reasoning tasks coordinated through adaptive feedback loops, eliminating reliance on consumer code, hand-crafted heuristics, or manual triage of build failures.

    \item \textbf{Model Context Protocol for tool-augmented code exploration.} We present an extensible framework enabling LLM agents to perform lazy, conversational queries against code analysis backends, thus transforming static analysis from a rigid preprocessing step into an adaptive reasoning resource.

    \item \textbf{Coverage-guided refinement} We demonstrate systematic improvement of harness quality through critic-agent collaboration, multi-criteria convergence detection, and cycle-aware exploration strategies that prevent semantic drift while maximizing achieved coverage.

    \item \textbf{Empirical validation on real-world Java libraries.} We evaluate our approach against established baselines including Jazzer's AutoFuzz and OSS-Fuzz-Gen, TODO: wir sind besser
\end{itemize}


\section{Preliminaries}

This section establishes the technical foundation for our approach, introducing coverage-guided fuzzing for managed runtimes, the role of large language models in code synthesis, and relevant baseline systems.

\subsection{Coverage-Guided Fuzzing and Harness Design}
\label{sec:prelim:fuzzing}

Coverage-guided fuzzing operates by repeatedly executing a program with generated inputs while monitoring code coverage feedback. Each execution provides feedback, typically in the form of newly covered branches or basic blocks, that guides subsequent input mutations toward unexplored program regions. This feedback loop enables fuzzers to systematically navigate complex input spaces and discover deep bugs that evade random testing.

Applying coverage-guided fuzzing to library code introduces a fundamental challenge: libraries expose APIs rather than standalone executables, requiring a \emph{fuzz harness} to mediate between the fuzzer and the target. A harness transforms unstructured byte streams from the fuzzer into valid API invocations by: (1)~parsing fuzz input into structured data types, (2)~constructing necessary object state and satisfying initialization preconditions, (3)~invoking target methods with derived arguments, and (4)~handling or observing exceptions and abnormal termination. The quality of this translation directly determines fuzzing effectiveness: shallow harnesses that merely parse inputs yield limited coverage, while well-designed harnesses that exercise complex API interactions expose deeper program logic and uncover latent bugs.

\subsection{Jazzer: Coverage-Guided Fuzzing for the JVM}
\label{sec:prelim:jazzer}

Jazzer~\cite{CITE:Jazzer} brings coverage-guided fuzzing to the Java Virtual Machine through a hybrid architecture combining native code instrumentation with JVM bytecode manipulation. The fuzzer executes as a native process that invokes Java methods via JNI, while a Java agent instruments target bytecode at class-load time to collect coverage feedback. This design enables efficient in-process fuzzing with fine-grained branch coverage, comparable to AFL and libFuzzer for native code.

Jazzer harnesses implement a standard interface that provides methods for consuming integers, strings, booleans, and other primitive types. Harness authors use this interface to construct valid inputs and orchestrate API calls. While manual harness creation remains common practice, Jazzer includes an AutoFuzz mode that leverages Java reflection to automatically generate harnesses. AutoFuzz discovers accessible constructors and methods, recursively building required objects and mapping fuzzer bytes to parameter types. 


\subsection{Large Language Models for Code Synthesis}
\label{sec:prelim:llm}

Large Language Models (LLMs) are transformer-based neural architectures trained on massive corpora of source code and natural language to predict and generate token sequences. Through next-token prediction over billions of parameters, LLMs internalize syntactic patterns, API usage idioms, and semantic relationships between code elements. When conditioned on appropriate context—such as function signatures, documentation, or partial implementations—LLMs can synthesize plausible code completions, generate test cases, or repair buggy programs~\cite{CITE:Codex,CITE:AlphaCode}.

Recent work has demonstrated that LLMs can be organized into multi-agent systems where distinct instances specialize in complementary subtasks~\cite{CITE:MultiAgentCode}. Rather than relying on a single monolithic prompt, multi-agent architectures decompose complex objectives into stages handled by specialized agents that communicate through structured message passing or shared state. For code generation tasks, this decomposition enables separation of concerns: one agent explores API documentation and infers usage patterns, another synthesizes implementations, while a third diagnoses compilation errors and proposes fixes. This division of labor mirrors human collaborative workflows and has been shown to improve both generation quality and success rates on challenging benchmarks~\cite{CITE:AgentBench}.

\subsection{Tool-Augmented LLM Reasoning}
\label{sec:prelim:tools}

While LLMs demonstrate impressive code understanding from pre-training, their knowledge is static and limited to patterns observed in training data. \emph{Tool-augmented reasoning} addresses this limitation by equipping LLMs with external capabilities they can invoke during generation~\cite{DBLP:conf/nips/SchickDSHWSCSW23:Toolformer,CITE:ToolLLM}. For code synthesis tasks, relevant tools include compilers (to validate syntax), test executors (to verify functional correctness), static analyzers (to extract API signatures), and documentation retrievers (to ground generation in current libraries).

The ReAct (Reasoning and Acting) paradigm~\cite{DBLP:conf/iclr/YaoZYDN023:ReAct} formalizes tool use as an interleaved process: the model alternates between reasoning steps (generating natural language explanations of its strategy) and action steps (invoking tools and observing outputs). This loop continues until the model determines it has sufficient information to complete the task. ReAct-style agents have demonstrated substantial improvements over direct prompting on tasks requiring information retrieval, calculation, or interaction with external systems.

\subsection{Baseline: OSS-Fuzz-Gen}
\label{sec:prelim:ossfuzzgen}

OSS-Fuzz-Gen~\cite{CITE:OSSFuzzGen} represents the current state-of-the-art in LLM-driven harness generation. It combines lightweight static analysis with iterative LLM prompting to synthesize harnesses for libraries integrated into Google's OSS-Fuzz continuous fuzzing service. The system operates in three phases: (1)~static analysis extracts function signatures and type information, (2)~an LLM generates candidate harnesses conditioned on templates encoding language-specific patterns (e.g., Java exception handling, C++ RAII), and (3)~compilation and short fuzzing trials filter invalid or low-coverage candidates.
OSS-Fuzz-Gen's Java extension provides language-specific prompt templates that guide harness structure for Java libraries. 




\section{Automated Harness Generation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth, height=4cm]{workflow.png}
    \caption{Multi-agent workflow architecture. Specialized agents collaborate through shared context to synthesize, validate, and iteratively refine fuzzing harnesses until convergence.}
    \label{fig:methodology:system-overview}
\end{figure*}

We present a multi-agent approach to automated fuzzing harness generation that addresses the central challenge of scaling library fuzzing without requiring consumer code or manual intervention. Our method decomposes the synthesis problem into specialized tasks, assigns each to a dedicated agent, and orchestrates their collaboration through a shared workspace that accumulates knowledge and convergence signals across iterations.

\subsection{Design Principles}

Our approach is guided by three foundational principles. First, we embrace \emph{tool-augmented reasoning}, where agents are equipped with structured query interfaces to code analysis backends rather than relying on pre-computed summaries or fixed templates. This enables adaptive exploration: agents decide what information to retrieve based on their evolving understanding of the target API. Second, we adopt \emph{natural communication between agents}, rejecting rigid structured representations in favor of natural language summaries that preserve reasoning nuance and reduce brittleness to schema changes. Third, we implement \emph{feedback-driven convergence}, where each synthesis-validation cycle produces observable signals (e.g., compilation errors, coverage deltas, crash reports) that inform subsequent refinement decisions.


\subsection{Workflow Architecture}

Figure~\ref{fig:methodology:system-overview} illustrates our staged workflow. The pipeline accepts a target specification, a library identifier and method signature, and produces a compilable, coverage-optimized harness through five primary stages: \emph{environment preparation}, \emph{exploratory research}, \emph{harness synthesis}, \emph{validation}, and \emph{adaptive refinement}.

The workflow is orchestrated as a state machine where each stage contributes updates to a shared context object. This context accumulates artifacts (harness code, dependencies, coverage metrics), diagnostic signals (compilation errors, uncovered branches), and convergence indicators (iteration counts, coverage plateaus). Transitions between stages are governed by success criteria and adaptive routers that may trigger refinement loops, restart generation, or terminate upon convergence. In the following subsections, we'll highlight each of those stages:

\subsection{Environment Preparation}

Given a Maven coordinate triple $(g, a, v)$, the system first establishes a compilable build context. We retrieve three artifact types from Maven Central: binary JARs (for fuzzing execution), source JARs (for code exploration), and Javadoc JARs (for API documentation). Dependency resolution follows POM metadata to recursively assemble a complete classpath, which is then embedded into a build template compatible with the Jazzer fuzzing framework.

In parallel, we extract structural metadata that guides agent reasoning. We employ Class Hierarchy Analysis (CHA)~\cite{DBLP:conf/ecoop/DeanGC95}, a conservative call graph construction algorithm, to compute reachable methods from the target entry point. CHA operates by resolving virtual method calls using class hierarchy information: for each call site, it considers all methods that could be invoked based on the receiver type's position in the inheritance lattice. While CHA may introduce imprecision by including unreachable paths (due to its conservative nature), it provides complete coverage guarantees, ensuring no executable method is omitted, which is critical for our fuzzing objectives. The resulting call graph is traversed breadth-first to a configurable depth, yielding a set of reachable methods with their signatures, declaring classes, and call-chain distances. This graph serves dual purposes: it informs the research agent about relevant API interactions and later provides refinement targets by identifying uncovered but reachable code regions.

By front-loading this analysis, we avoid redundant computation across refinement iterations and provide agents with a structural roadmap of the code under test.

\subsection{Tool-Augmented Code Exploration}

A central challenge in harness synthesis is understanding the target API's contracts, preconditions, and typical usage patterns without access to consumer code. Prior approaches either rely on pre-computed knowledge bases (which quickly become stale) (CITE) or perform exhaustive static analysis (which scales poorly) (CITE). We address this through \emph{conversational code exploration}: a research agent equipped with structured query tools navigates the codebase autonomously, gathering fuzzing-relevant insights on demand.

To enable this adaptive exploration, we introduce a \emph{Model Context Protocol (MCP)} architecture that decouples code analysis capabilities from agent reasoning logic. This separation offers three key advantages: (1)~\textbf{Modularity}—analysis backends can be upgraded or replaced without modifying agent prompts; (2)~\textbf{Lazy evaluation}—agents retrieve only the information they deem relevant, reducing token consumption; and (3)~\textbf{Composability}—multiple analysis modalities can be integrated seamlessly.

We implement three specialized MCP servers, each exposing a domain-specific query interface:

\begin{itemize}
    \item \textbf{Documentation Server.} Indexes Javadoc archives to provide on-demand retrieval of API contracts. Given a fully qualified class name and optional method signature, it returns structured metadata including parameter types, return types, exception specifications, and natural-language descriptions. This enables agents to understand method semantics without parsing raw bytecode or decompiled sources.

    \item \textbf{Call Graph Server.} Exposes the CHA-derived call graph as a queryable structure. Agents submit queries specifying a root method and depth limit, receiving a hierarchical tree of reachable methods with invocation relationships. This tool proves particularly valuable during research, where agents use it to discover deep interaction patterns, identify complex initialization sequences, and reason about API dependencies.

    \item \textbf{Source Code Server.} Indexes the unpacked source JAR to support fine-grained implementation retrieval. Agents query by class and method signature, receiving the full implementation along with surrounding context (e.g., class-level fields, inner classes, adjacent methods). Unlike static analysis tools that extract rigid features, this server provides raw source text, allowing LLMs to apply their learned code understanding directly.
\end{itemize}

Each server runs as a standalone subprocess with standard I/O transport, enabling asynchronous tool invocation without coupling to specific analysis implementations. This architecture transforms what would be a brittle pipeline of static analysis passes into a flexible reasoning environment where agents autonomously decide which code aspects to examine.

All research agent operates in a ReAct loop~\cite{DBLP:conf/iclr/YaoZYDN023:ReAct}, alternating between reasoning steps and tool invocations. Unlike traditional static analysis that processes all code uniformly, the agent dynamically constructs its exploration strategy based on evolving understanding. For instance, when encountering a method with complex preconditions, it may: (1)~query the documentation server for contract specifications, (2)~traverse the call graph to identify initialization helpers, and (3)~retrieve source implementations of those helpers to understand setup requirements.

Critically, we instruct the agent to prioritize \emph{fuzzing-oriented knowledge}: valid input ranges, boundary conditions, exception paths, and API call dependencies. This focus prevents exhaustive documentation harvesting and concentrates effort on information that directly informs harness quality. The agent's output is a natural-language research summary, a deliberate choice that preserves reasoning context and avoids brittleness of structured formats prone to parsing failures. This summary is passed directly to the generation agent, maintaining a chain of reasoning across synthesis stages.

\subsection{Harness Synthesis and Validation}

Given the research summary, a generation agent synthesizes an initial harness candidate. This agent receives detailed templates explaining the fuzzing framework's interface alongside the research findings. It proposes a harness implementation, reasons about potential issues, and may invoke code retrieval tools for clarification. The output includes harness source code and a dependency manifest.

Synthesis is decoupled from validation: the generation agent focuses on producing semantically plausible code without attempting compilation. Validation is delegated to a separate compilation agent that embeds the harness into the prepared build template and invokes the build system. If compilation fails, this agent diagnoses the error by parsing messages to identify root causes such as unresolved symbols or type mismatches and formulates repairs. For tractable errors (missing imports, dependency omissions), it applies automatic fixes; for complex failures (API misuse, version incompatibilities), it may query tools to refine its understanding before retrying.

This separation of concerns reflects a key insight: generation and debugging require distinct reasoning modes. Generation benefits from creative exploration of API usage patterns, while debugging demands systematic diagnosis and targeted correction. By assigning these tasks to specialized agents, we improve both synthesis quality and repair success rates.

\subsection{Coverage-Guided Refinement}

Upon successful compilation, the harness enters a fuzzing phase where it executes under coverage instrumentation to measure code coverage. We configure time-bounded fuzzing (rather than iteration-bounded) to ensure consistent resource budgets across targets with varying complexity. Fuzzing produces three critical signals: line and branch coverage percentages, a map of uncovered code regions, and any crash-triggering inputs with stack traces.

These signals drive a refinement loop orchestrated by a critic-agent pair. The critic analyzes coverage gaps—uncovered branches, unexplored call paths, missing API interactions—and estimates improvement potential. It identifies specific weaknesses (e.g., insufficient input diversity, missing initialization sequences) and assigns a confidence score indicating refinement feasibility. If promising opportunities exist, a refinement agent modifies the harness to address the identified gaps, leveraging code exploration tools to understand uncovered regions.

The refined harness is recompiled and fuzzed, closing the feedback loop. This cycle repeats until convergence criteria are met: coverage improvement plateaus (less than 1\% gain for three consecutive iterations), iteration budgets are exhausted, or the critic's confidence drops below a threshold. To prevent oscillation between equivalent harnesses, we maintain content-addressed storage with hash-based deduplication, detecting cycles and prompting agents to explore alternative strategies.

\subsection{Convergence and Reproducibility}

Termination is governed by a multi-criteria policy that balances coverage maximization with resource constraints. We detect convergence through coverage plateaus (indicating local optima), enforce hard iteration limits (bounding compute budgets), and monitor for duplicate harnesses (preventing infinite loops). Upon termination, the system records comprehensive metadata—iteration traces, agent decisions, coverage evolution—enabling post-hoc analysis of synthesis trajectories.

To support reproducibility and experimentation, we implement checkpoint-resume mechanisms that snapshot workflow state at key stages. This allows researchers to resume from successful compilation points, varying only refinement strategies or fuzzing parameters without re-executing expensive analysis and generation phases. All artifacts are organized in session-based directories with content-addressed storage, ensuring that identical harnesses are deduplicated while preserving their evolution history through timeline metadata.

\subsection{Implementation}

Our prototype system is implemented in Python using the LangGraph framework for state machine orchestration and LangChain for LLM integration. We support both OpenAI (GPT-4) and Anthropic (Claude 3.5 Sonnet) models as reasoning backends, with temperature parameters tuned for code generation tasks (default: 0.3). Agent communication with analysis backends follows the Model Context Protocol (MCP), a standardized interface for tool-augmented LLM systems. Three MCP servers provide documentation retrieval (parsing Javadoc JARs), call graph queries (using SootUp~\cite{DBLP:conf/icse/SchottKB25:SootUp} with Class Hierarchy Analysis), and source code extraction (indexing unpacked source JARs).

Coverage measurement integrates Jazzer's native instrumentation with a standalone JaCoCo server that exposes coverage metrics via REST API. We employ dual filtering—Jazzer's bytecode instrumentation scope and JaCoCo's measurement filter—to isolate target library coverage from framework initialization overhead. The compilation stage uses Gradle templates with automatic dependency injection, and harness code is instrumented with JaCoCo control hooks to ensure accurate per-run coverage isolation. Artifact storage follows a session-based schema with SHA-256 content addressing for harness deduplication, timeline tracking for evolution reconstruction, and JSON metadata for convergence analysis.


\section{Evaluation}





\section{Related Work}
\label{sec:related-work}

This section reviews existing approaches to automated harness generation, positioning our work within the broader research landscape through a comprehensive survey of classical and modern techniques.

\subsection{Classical Approaches to Harness Generation}
\label{subsec:classical-harness-generation}



Different traditions of program analysis have shaped how fuzzing harnesses are constructed. A first line of work can be described as \textbf{(1) usage-based generation}, where valid API calls are mined from existing consumer code or unit tests and then repurposed to build drivers. This strategy captures realistic interaction patterns between libraries and clients, as demonstrated by systems that slice client code into reusable API snippets~\cite{DBLP:conf/sigsoft/BabicBCIKKLSW19:FUDGE}, construct harness stubs from API dependence graphs extracted from consumer projects~\cite{DBLP:conf/uss/IspoglouAMP20:FuzzGen}, inject fuzzed inputs into test cases while preserving their order~\cite{DBLP:conf/sp/JeongJYMKJKSH23:UTopia}, or aggregate traces and snippets from package repositories~\cite{DBLP:journals/pacmse/WuNH25:WildSync, DBLP:conf/icse/ZhangZLWLZJ23:Daisy}.

In contrast, \textbf{(2) structure- or API-based generation} avoids reliance on external code and derives harnesses directly from type signatures, header files, or interface specifications. Approaches in this category focus on risky functions such as dereferences~\cite{DBLP:conf/icse:ZhangLMZ021:IntelliGen}, build dataflow graphs to capture API interactions~\cite{DBLP:conf/icse/GreenA22:GraphFuzz}, adapt this principle to specific domains such as OEM Android components and JNI bindings~\cite{DBLP:conf/ccs/ChenXLWC23:Hopper, DBLP:conf/uss/IspoglouAMP20:FuzzGen, DBLP:conf/issta/XiongDCQWSZ24:Atlas}, or introduce intermediate representations, API-flow clustering, and constraint learning to enable application to large-scale or closed-source libraries~\cite{DBLP:journals/pacmse/ToffaliniBTP25:LibErator, DBLP:conf/icse/ShermanN25:OGHarn, DBLP:conf/ndss/0007ZLSZLQ25:NEXZZER}.

Finally, there is \textbf{(3) evolutionary or feedback-driven generation}, where harnesses are refined iteratively based on runtime signals such as coverage or oracle checks. This approach uses automaton learning on API usage patterns~\cite{DBLP:conf/uss/ZhangLZZZZXLL0H23:Rubick}, ranks and selects candidate harnesses after short fuzzing trials~\cite{DBLP:journals/pacmse/ToffaliniBTP25:LibErator}, filters out misuses through relation learning~\cite{DBLP:conf/ndss/0007ZLSZLQ25:NEXZZER}, or validates candidates using compile-time and runtime oracles~\cite{DBLP:conf/icse/ShermanN25:OGHarn}. These systems demonstrate how feedback loops can drive continuous improvement, in contrast to static one-shot generation.

\subsection{LLM-based Approaches}
\label{subsec:llm-approaches}


With the availability of large language models, harness generation has also been explored from a learning perspective. One stream of work takes the form of \textbf{(1) feasibility studies}, which probe the capabilities and limitations of LLMs in fuzzing. These studies evaluate different prompting strategies across benchmarks~\cite{DBLP:conf/issta/ZhangZBLMXLSL24:HowEffectiveAreThey} and discuss fundamental obstacles such as semantic drift and the absence of reliable oracles~\cite{DBLP:conf/sigsoft/Jiang0MCZSWFWLZ24:WhenFuzzingMeetsLLMs}.

Beyond these exploratory efforts, a growing body of research investigates \textbf{(2) automatic harness synthesis with LLMs}. Systems in this category introduce coverage-guided prompt mutation for iterative refinement~\cite{DBLP:conf/ccs/LyuXCC24:PromptFuzz}, augment LLM reasoning with knowledge graphs of API relations~\cite{DBLP:conf/icse/XuMZZCHLW25:CKGFuzzer}, address binary-only interfaces with agent-based synthesis~\cite{DBLP:journals/corr/abs-2507-15058:LibLMFuzz}, target unsafe Rust APIs~\cite{DBLP:journals/corr/abs-2506-15648:deepSURF}, integrate CVE metadata to guide call-chain harnesses~\cite{DBLP:journals/corr/abs-2505-03425:HGFuzzer}, or employ constraint dependency graphs for Python libraries~\cite{DBLP:journals/cybersec/LiuLZZLL25:LLM4TDG}.

A particularly promising movement is \textbf{(3) hybrid and multi-agent architectures}, where LLMs are embedded into broader analytic ecosystems. Approaches in this category orchestrate several agents around a knowledge graph~\cite{DBLP:conf/icse/XuMZZCHLW25:CKGFuzzer}, combine LLM-based repair with solver-driven scheduling~\cite{DBLP:journals/corr/abs-2507-18289:Scheduzz}, or integrate LLM reasoning into static analysis pipelines built on CodeQL and Tree-Sitter~\cite{DBLP:journals/corr/abs-2505-03425:HGFuzzer}. These works illustrate the potential of combining generative capabilities with structured analysis and feedback mechanisms.


\section{Conclusion and Outlook}
TODO


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.