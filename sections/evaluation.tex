\section{Evaluation}%
\label{sec:evaluation}

We evaluate our harness generation approach on widely used Maven libraries, examining achieved coverage, computational costs, and agent behavior patterns. Our evaluation demonstrates that the approach produces competitive harnesses to existing baselines and techniques while maintaining practical generation costs. Additionally, during the 12-hour fuzzing campaigns, our harnesses uncovered multiple previously unknown bugs in mature libraries, validating their effectiveness in real-world testing scenarios.

\subsection{Experimental Setup}%
\label{subsec:exp-setup}
\input{tables/benchmarks.tex}
We evaluate on seven target methods from six widely-deployed Java libraries (Table~\ref{tab:benchmarks}). The selected targets span parsers (commons-cli, jsoup, antlr4), JSON libraries (gson, jackson-databind), and core utilities (guava).
All selected target methods have existing harnesses in OSS-Fuzz~\cite{CITE:OSSFuzz}, which serve as our primary baseline. We additionally compare against Jazzer AutoFuzz~\cite{CITE:Jazzer}, which uses Java reflection for automatic harness generation. For LLM-based comparison, we attempted to use OSS-Fuzz-Gen~\cite{CITE:OSSFuzzGen}, but encountered implementation issues, primarily in model output parsing, that prevented successful harness generation for the selected targets Java targets.
\par
To measure the effectiveness of fuzzing the targeted method, we measure coverage under two configurations: (1)~method-targeted coverage activates only during target method execution (Section~\ref{subsec:static-analysis}), focusing metrics on target behavior; (2)~full target-scope coverage uses standard JaCoCo instrumentation across the entire library for fair baseline comparison.
All campaigns are run for 12 hours per target with a single fuzzing thread and an empty seed corpus. 
\subsection{Coverage Effectiveness}%
\label{subsec:coverage-effectiveness}

\input{figures/coverage-overall.tex}

Figure~\ref{fig:coverage-comparison} shows line coverage over time across five Java libraries. The top row compares method-targeted coverage for our generated harnesses against the OSS-Fuzz baseline, demonstrating focused execution of target method logic. The bottom row shows full target-scope coverage comparing our approach against AutoFuzz.
% TODO After plots are finished
The generated harnesses match or outperform coverage across all evaluated targets. Under full target-scope instrumentation, our harnesses reach [TODO: X\%] average coverage compared to AutoFuzz's [TODO: Y\%], representing a [TODO: Z\%] relative improvement.
%
%\paragraph{Coverage Trajectory Analysis}
%Beyond final coverage values, the temporal dynamics reveal important differences in how harnesses explore target code. Our harnesses typically reach 80\% of their final coverage within the first hour, indicating effective initial synthesis that exercises core API paths immediately. In contrast, AutoFuzz exhibits slower convergence, often requiring 2-3 hours to plateau. This suggests that reflection-based type instantiation discovers shallow API interactions quickly but struggles to synthesize complex call sequences required for deeper coverage.
%
%Notably, gson and jackson-databind show continued coverage growth throughout the 12-hour campaign for our harnesses, while AutoFuzz plateaus within 3-4 hours. This sustained exploration indicates that our harnesses exercise diverse input patterns that trigger different code paths, whereas AutoFuzz's deterministic type coercion limits input space diversity.
%
\par
\input{figures/case-study-coverage.tex}
To demonstrate the benefits of targeted harness generation, we evaluate ANTLR4, which has two target methods in our benchmark. The existing OSS-Fuzz harness~\footnote{https://github.com/google/oss-fuzz/blob/master/projects/antlr4-java/GrammarFuzzer.java} exercises both methods sequentially: it creates a Grammar object from fuzzed input, then invokes createParserInterpreter on the resulting grammar. This sequential dependency means the parser interpreter is only reached when grammar creation succeeds without throwing exceptions.
We compare our automatically generated harnesses (each targeting one method individually) against the OSS-Fuzz harness measured with method-targeted coverage scoped to each target method. For the Grammar constructor (Figure~\ref{fig:antlr4-coverage}(a)), we evaluate both the unmodified OSS-Fuzz harness and a manually edited variant that only creates the grammar without invoking the parser interpreter. For createParserInterpreter (Figure~\ref{fig:antlr4-coverage}(b)), we evaluate only the unmodified OSS-Fuzz harness.
Our generated harnesses outperform the OSS-Fuzz baseline in both scenarios. Most notably, the OSS-Fuzz harness achieves 0\% coverage for createParserInterpreter throughout the campaign, indicating that grammar creation consistently throws exceptions before reaching the parser interpreter call. In contrast, our generated harness successfully exercises this method by synthesizing inputs that satisfy the grammar constructor's preconditions. Note that both ANTLR4 campaigns encountered a Jazzer timeout at 30 minutes that terminates execution; we were unable to configure Jazzer to continue, though the coverage trends before termination clearly demonstrate the performance difference.
%
\subsection{Bug Discovery}%
\label{subsec:bug-discovery}
Beyond coverage metrics, we examine whether our generated harnesses discover actual bugs during fuzzing campaigns. Jazzer reports crashes through its exception handling infrastructure, distinguishing between genuine crashes (uncaught exceptions indicating bugs) and caught exceptions that represent normal control flow. This presents are core challenge for the harness generation, as the generated harnesses must avoid over-catching exceptions that would mask real bugs while also prevent spurious crashes from expected error handling.
Across the 12-hour fuzzing campaigns, the generated harnesses triggered a total of 14 crashes in two libraries. After manual investigation we determine, that all reported crashes represent genuine bugs in the target library. The harnesses correctly identified uncaught exceptions rather than reporting false positives from expected exception handling. Manual triage revealed 3 unique bugs:

\begin{itemize}
\item \textbf{commons-cli}: Two distinct null pointer exceptions in option parsing logic, triggered by edge-case combinations of option configurations and malformed arguments. The crashes manifest in both long-option and short-option code paths, with 12 total crash artifacts reducing to 2 unique root causes.
\item \textbf{jsoup}: One index-out-of-bounds exception in HTML tree building logic, triggered by complex malformed HTML input (~1KB). This crash represents a potential denial-of-service vector, as attackers could craft HTML to crash the parser. Two crash artifacts correspond to the same underlying bug.
\end{itemize}

We are currently coordinating responsible disclosure with library maintainers. The commons-cli bugs represent robustness issues, while the jsoup bug has higher severity due to its potential impact on server-side HTML processing.
These results demonstrate that our automatically-generated harnesses achieve sufficient input diversity and API coverage to discover real bugs in mature, widely-deployed libraries that already have existing harnesses in the OSS-Fuzz ecosystem. The fact that Jazzer reported zero false positives highlights the effectiveness of the harness generation in balancing exception handling to expose genuine bugs without over-catching.
%
\subsection{Generation Costs and Agent Behavior}%
\label{subsec:generation-costs}
%
\input{tables/harness-generation-cost.tex}
%
Table~\ref{tab:generation-cost} details the computational costs and agent activity patterns across the five main targets. Harness generation costs range from \$1.34 (guava) to \$6.25 (commons-cli), with an average of \$3.20 per harness. Generation completes in an average of 599 seconds (~10 minutes), making the approach practical for integration into iterative fuzzing workflows.
Token consumption directly correlates with workflow complexity. The observed diversity in workflow iterations and agent loops indicates effective adaptation to target-specific characteristics, with more complex APIs requiring additional research and refinement cycles while simpler targets lead to quicker convergence. 
\par
Comparing usage patterns between the research agent and the generation agent reveals that the initial report contains most of the necessary information for synthesis requiring on average only $3.1$ iteration until the first harness is synthesized. Additionally, the patching agent invocation pattern reveals that, in most cases, the initial synthesis is already correct and only few targets require repair iterations with all evaluated harnesses compiling successfully after at most $6$ iterations.
The coverage analysis and refinement agents show high variance reflecting target-specific characteristics.
This adaptive behavior demonstrates that our agent-based termination successfully distinguishes between targets where refinement yields benefits and those where additional iteration would waste resources.



