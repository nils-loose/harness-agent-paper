\section{Evaluation}%
\label{sec:evaluation}

We evaluate our harness generation approach on widely used Maven libraries, examining achieved coverage, computational costs, and agent behavior patterns. Our evaluation demonstrates that the approach produces competitive harnesses to existing baselines and techniques while maintaining practical generation costs. Additionally, during the 12-hour fuzzing campaigns, our harnesses uncovered multiple previously unknown bugs in mature libraries, validating their effectiveness in real-world testing scenarios.

\subsection{Experimental Setup}%
\label{subsec:exp-setup}
\input{tables/benchmarks.tex}
We evaluate on seven target methods from six widely-deployed Java libraries (Table~\ref{tab:benchmarks}). The selected targets span parsers (commons-cli, jsoup, antlr4), JSON libraries (gson, jackson-databind), and core utilities (guava).
All selected target methods have existing harnesses in OSS-Fuzz~\cite{CITE:OSSFuzz}, which serve as our primary baseline. We additionally compare against Jazzer AutoFuzz~\cite{CITE:Jazzer}, which uses Java reflection for automatic harness generation. For LLM-based comparison, we attempted to use OSS-Fuzz-Gen~\cite{CITE:OSSFuzzGen}, but encountered implementation issues, primarily in model output parsing, that prevented successful harness generation for the selected targets Java targets.
\par
To measure the effectiveness of fuzzing the targeted method, we measure coverage under two configurations: (1)~method-targeted coverage activates only during target method execution (Section~\ref{subsec:static-analysis}), focusing metrics on target behavior; (2)~full target-scope coverage uses standard JaCoCo instrumentation across the entire library for fair baseline comparison.
All campaigns are run for 12 hours per target with a single fuzzing thread and an empty seed corpus. 
\subsection{Coverage Effectiveness}%
\label{subsec:coverage-effectiveness}

\input{figures/coverage-overall.tex}

Figure~\ref{fig:coverage-comparison} shows line coverage over time across five Java libraries. The top row compares method-targeted coverage for our generated harnesses against the OSS-Fuzz baseline, demonstrating focused execution of target method logic. The bottom row shows full target-scope coverage comparing our approach against AutoFuzz.
% TODO After plots are finished
The generated harnesses match or outperform coverage across all evaluated targets. Under full target-scope instrumentation, our harnesses reach [TODO: X\%] average coverage compared to AutoFuzz's [TODO: Y\%], representing a [TODO: Z\%] relative improvement.
%
%\paragraph{Coverage Trajectory Analysis}
%Beyond final coverage values, the temporal dynamics reveal important differences in how harnesses explore target code. Our harnesses typically reach 80\% of their final coverage within the first hour, indicating effective initial synthesis that exercises core API paths immediately. In contrast, AutoFuzz exhibits slower convergence, often requiring 2-3 hours to plateau. This suggests that reflection-based type instantiation discovers shallow API interactions quickly but struggles to synthesize complex call sequences required for deeper coverage.
%
%Notably, gson and jackson-databind show continued coverage growth throughout the 12-hour campaign for our harnesses, while AutoFuzz plateaus within 3-4 hours. This sustained exploration indicates that our harnesses exercise diverse input patterns that trigger different code paths, whereas AutoFuzz's deterministic type coercion limits input space diversity.
%
\par
\input{figures/case-study-coverage.tex}
To demonstrate the effect of targeted harness generation we evaluate the existing OSS-Fuzz harness for ANTLR4~\footnote{https://github.com/google/oss-fuzz/blob/master/projects/antlr4-java/GrammarFuzzer.java} with targeted method coverage. The OSS-Fuzz harness exercises multiple methods in a single driver, including grammar and subsequently parser interpreter creation. Notably, the effective tesiting of the parser interpreter relies on the prior successful creation of a valid grammar object. To evlaute the respective coverage we modify the harness once by only creating the grammar object (Figure~\ref{fig:antlr4-coverage}(a)) and once by using the harness as-is (Figure~\ref{fig:antlr4-coverage}(b)). The coverage for each harness is then measured with method-targeted coverage for the respective target method. Additionally, we compare against two of our automatically generated harnesses that each target one of the two methods individually. The results show that both of our generated harnesses outperform the existing harness in both scenarios. For the unmodified harness it is apparent that it does not even reach the target method createParserInterpreter as the coverage remains at 0\% throughout the fuzzing campaign whereas our generated handness succesfully exersised this method. While both these campains were run with identical confugrations as described above, consistently after 30 minutes jazzer runs a test an artifact that triggers a timeout causing jazzer to terminate. We were unable to configure jazzer to continue running. Yet, the general trend is observable.
%
\subsection{Bug Discovery}%
\label{subsec:bug-discovery}
Beyond coverage metrics, we examine whether our generated harnesses discover actual bugs during fuzzing campaigns. Jazzer reports crashes through its exception handling infrastructure, distinguishing between genuine crashes (uncaught exceptions indicating bugs) and caught exceptions that represent normal control flow. This presents are core challenge for the harness generation, as the generated harnesses must avoid over-catching exceptions that would mask real bugs while also prevent spurious crashes from expected error handling.
Across the 12-hour fuzzing campaigns, the generated harnesses triggered a total of 14 crashes in two libraries. After manual investigation we determine, that all reported crashes represent genuine bugs in the target library. The harnesses correctly identified uncaught exceptions rather than reporting false positives from expected exception handling. Manual triage revealed 3 unique bugs:

\begin{itemize}
\item \textbf{commons-cli}: Two distinct null pointer exceptions in option parsing logic, triggered by edge-case combinations of option configurations and malformed arguments. The crashes manifest in both long-option and short-option code paths, with 12 total crash artifacts reducing to 2 unique root causes.
\item \textbf{jsoup}: One index-out-of-bounds exception in HTML tree building logic, triggered by complex malformed HTML input (~1KB). This crash represents a potential denial-of-service vector, as attackers could craft HTML to crash the parser. Two crash artifacts correspond to the same underlying bug.
\end{itemize}

We are currently coordinating responsible disclosure with library maintainers. The commons-cli bugs represent robustness issues, while the jsoup bug has higher severity due to its potential impact on server-side HTML processing.
These results demonstrate that our automatically-generated harnesses achieve sufficient input diversity and API coverage to discover real bugs in mature, widely-deployed libraries that already have existing harnesses in the OSS-Fuzz ecosystem. The fact that Jazzer reported zero false positives highlights the effectiveness of the harness generation in balancing exception handling to expose genuine bugs without over-catching.
%
\subsection{Generation Costs and Agent Behavior}%
\label{subsec:generation-costs}
%
\input{tables/harness-generation-cost.tex}
%
Table~\ref{tab:generation-cost} details the computational costs and agent activity patterns across the five main targets. Harness generation costs range from \$1.34 (guava) to \$6.25 (commons-cli), with an average of \$3.20 per harness. Generation completes in an average of 599 seconds (~10 minutes), making the approach practical for integration into iterative fuzzing workflows.
Token consumption directly correlates with workflow complexity. The observed diversity in workflow iterations and agent loops indicates effective adaptation to target-specific characteristics, with more complex APIs requiring additional research and refinement cycles while simpler targets lead to quicker convergence. 
\par
Comparing usage patterns between the research agent and the generation agent reveals that the initial report contains most of the necessary information for synthesis requiring on average only $3.1$ iteration until the first harness is synthesized. Additionally, the patching agent invocation pattern reveals that, in most cases, the initial synthesis is already correct and only few targets require repair iterations with all evaluated harnesses compiling successfully after at most $6$ iterations.
The coverage analysis and refinement agents show high variance reflecting target-specific characteristics.
This adaptive behavior demonstrates that our agent-based termination successfully distinguishes between targets where refinement yields benefits and those where additional iteration would waste resources.



